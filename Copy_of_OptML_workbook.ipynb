{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of OptML_workbook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iE2eWAP4pk04"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpN2CxxThJlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authors:\n",
        "## Edgar Ivan Sanchez Medina    https://www.mpi-magdeburg.mpg.de/person/103552/2316\n",
        "## Antonio del Rio Chanona      https://www.imperial.ac.uk/people/a.del-rio-chanona\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvd4Do2tsb_R",
        "colab_type": "text"
      },
      "source": [
        "Along this notebook you will find <font color='blue'>text in blue that describe the **coding tasks** that you have to implement.</font> Within the cell code, your implementation needs to be between the line:\n",
        "\n",
        "`#-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#`\n",
        "\n",
        "and the line:\n",
        "\n",
        "`#-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#-#`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9wqS195hURt",
        "colab_type": "text"
      },
      "source": [
        "# **1. Optimization basics**\n",
        "\n",
        "*Optimization methods are the core engine of Machine Learning methods, they enable machine learning algorithms to learn from data.*\n",
        "\n",
        "<font color='red'>Una gif de la trayectoria de optimizacion con el valor del gradiente</font>\n",
        "\n",
        "*   Optimization refers to the selection of the **best point** that minimizes or maximizes an objective function that we specified.\n",
        "\n",
        "* There might be many **local optima** in a function. Usually we are interested in the **global optimum**.\n",
        "\n",
        "* In general, in Machine Learning the most common optimization problem for parameter estimation is an **unconstraint problem**.\n",
        "\n",
        "*   The **necessary optimality condition** in an unconstraint optimization problem is that $\\frac{df}{dx} = 0$, or in multiple dimensions, when $\\nabla f = 0$. Therefore, we need to calculate the gradient of the cost function (objective function) with respect to the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3VL3VgwxBHi",
        "colab_type": "text"
      },
      "source": [
        "# **2. Gradient approximation**\n",
        "\n",
        "One of the most common ways to approximate the gradient of a function numerically is the **finite differences method**. There exist mainly three type of finite difference approximations:\n",
        "\n",
        "* Backward difference  $f'(x) \\approx \\frac{f(x_k) - f(x_k - \\epsilon)}{\\epsilon}$\n",
        "* Forward difference  $f'(x) \\approx \\frac{f(x_k + \\epsilon) - f(x_k)}{\\epsilon}$\n",
        "* Central difference $f'(x) \\approx \\frac{f(x_k + \\frac{\\epsilon}{2}) - f(x_k - \\frac{\\epsilon}{2})}{\\epsilon}$\n",
        "\n",
        "However, the central difference approximation gives the most accurate one among these three. Therefore, let's implement that one here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVW_r4QWm7ND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################    \n",
        "# --- Forward finite differences --- #\n",
        "######################################\n",
        "\n",
        "def forward_finite_diff(f, x):\n",
        "      '''\n",
        "      Forward finite differences approximation.\n",
        "      INPUTS:\n",
        "          f  : Function\n",
        "          x  : Position where to approximate the gradient\n",
        "      OUTPUTS:\n",
        "          grad: Approximation of the gradient of f at x \n",
        "      '''\n",
        "      \n",
        "      dim = x.shape[0]\n",
        "      # Step-size is taken as the square root of the machine precision\n",
        "      eps  = np.sqrt(np.finfo(float).eps) \n",
        "      grad = np.zeros((1,dim))\n",
        "        \n",
        "      for i in range(dim):\n",
        "          e           = np.zeros((1,dim))\n",
        "          e[0,i]        = eps\n",
        "          grad_approx = (f(x + e) - f(x))/eps\n",
        "          grad[0,i]     = grad_approx\n",
        "\n",
        "      return grad\n",
        "\n",
        "# Note: this code is for educational purposes, vectorization not implemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bzEqcn4rOfq",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Implement the **central finite differences**.</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNLK5HJ3hXFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################    \n",
        "# --- Central finite differences --- #\n",
        "######################################\n",
        "\n",
        "def central_finite_diff(f, x):\n",
        "      '''\n",
        "      Central finite differences approximation.\n",
        "      INPUTS:\n",
        "          f  : Function\n",
        "          x  : Position where to approximate the gradient\n",
        "      OUTPUTS:\n",
        "          grad: Approximation of the gradient of f at x \n",
        "      '''\n",
        "      \n",
        "      dim = x.shape[0]\n",
        "      # Step-size is taken as the square root of the machine precision\n",
        "      eps  = np.sqrt(np.finfo(float).eps) \n",
        "      grad = np.zeros((1,dim))\n",
        "\n",
        "      #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#  \n",
        "      \n",
        "      #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#-#\n",
        "\n",
        "      return grad\n",
        "\n",
        "# Note: this code is for educational purposes, vectorization not implemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tToO26bMXBJd",
        "colab_type": "text"
      },
      "source": [
        "#### Prueba tu codigo aqui"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gYFOXwUXAF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "abe91bcf-2257-4d39-bbe3-7e2c0fe58085"
      },
      "source": [
        "###########################\n",
        "# --- trial functions --- # \n",
        "###########################\n",
        "'''\n",
        "These functions are to try the finite difference functions above\n",
        "'''\n",
        "\n",
        "def x_2(x):\n",
        "    return x@x.T\n",
        "\n",
        "def exp_sum(x):\n",
        "    return np.sum(np.exp(x))\n",
        "\n",
        "def log_lin(x):\n",
        "    return np.sum(x+np.log(x))\n",
        "\n",
        "#####################################################\n",
        "# --- try your finite difference algorithm here --- # \n",
        "#####################################################\n",
        "\n",
        "f_trials = [x_2, exp_sum, log_lin]\n",
        "x_trial  = np.array([1.,5.,2.5])\n",
        "\n",
        "for func_i in f_trials:\n",
        "    print(central_finite_diff(func_i, x_trial))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0.]]\n",
            "[[0. 0. 0.]]\n",
            "[[0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro56nor6XGNH",
        "colab_type": "text"
      },
      "source": [
        "#### Resultados esperados:\n",
        "\n",
        "$$f({\\bf x}):={\\bf x}^2 \\quad \\nabla f({\\bf x})=2{\\bf x}=[1, 10, 5]$$\n",
        "\n",
        "$$f({\\bf x}):=\\sum_i\\text{exp}( x_i) \\quad \\nabla f({\\bf x})=[\\text{exp}(x_1), \\text{exp}( x_2),..., \\text{exp}( x_n)]=[2.71, 148.41, 12.18]$$\n",
        "\n",
        "$$f({\\bf x}):=\\sum_i x_i+\\text{log}(x_i) \\quad \\nabla f({\\bf x})=[1+1/x_1, 1+1/x_2,..., 1+1/x_n]=[2, 1.2, 1.4]$$\n",
        "\n",
        "**Remember**: $\\nabla f({\\bf x})=[\\frac{\\partial f}{\\partial x_1}({\\bf x}),...,\\frac{\\partial f}{\\partial x_n}({\\bf x})]^T$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhLEwIfrJzCp",
        "colab_type": "text"
      },
      "source": [
        "There are other techniques that take more than two-points information to approximate the gradient, so-called **higher-order methods**. For example, the five-point methods, which takes information from more surronding points to approximate the gradient of the function more precisely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chFdsOiqJ7gE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################\n",
        "# --- Central finite differences 5 points --- # \n",
        "###############################################\n",
        "\n",
        "def central_finite_diff5(f, x):\n",
        "      '''\n",
        "      Five-points method for central finite differences.\n",
        "      INPUTS:\n",
        "          f  : Function\n",
        "          x  : Position where to approximate the gradient\n",
        "      OUTPUTS:\n",
        "          grad: Approximation of the gradient of f at x \n",
        "      '''\n",
        "      dim = x.shape[1]\n",
        "      # Step-size is taken as the square root of the machine precision\n",
        "      eps  = np.sqrt(np.finfo(float).eps) \n",
        "      grad = np.zeros((1,dim))\n",
        "        \n",
        "      for i in range(dim):\n",
        "          e           = np.zeros((1,dim))\n",
        "          e[0,i]      = eps\n",
        "          grad_approx = (f(x - 2*e) - 8*f(x - e) + 8*f(x + e) - f(x + 2*e) )/(12*eps) \n",
        "          \n",
        "          grad[0,i]     = grad_approx\n",
        "        \n",
        "      return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX4DprAMKqC-",
        "colab_type": "text"
      },
      "source": [
        "# **3. Optimization in Machine Learning**\n",
        "\n",
        "<font color='red'>Tal vez cambiar a la notacion original de f y x</font>\n",
        "\n",
        "In Machine Learning we have several common cost functions. They measure the deviation of our model predictions from the real values. And since they can be applied to a wide range of problems, it is worth calculating their exact derivatives and use this exact information for their optimization.\n",
        "\n",
        "Some of these loss functions for **regression**:\n",
        "\n",
        "Loss function | Equation\n",
        "--- | ---\n",
        "Mean Squared Error | $$\\frac{1}{m} \\sum_{i=1}^{m} (y_i-\\hat{y_i})^2 $$\n",
        "Mean Absolute Error | $$\\frac{1}{m} \\sum_{i=1}^{m} |y_i-\\hat{y_i}| $$\n",
        "Mean Absolute Percentage Error | $$\\frac{100}{m} \\sum_{i=1}^{m} \\frac{|y_i-\\hat{y_i}|}{y_i} $$\n",
        "Mean Squared Logarithmic Error | $$\\frac{1}{m} \\sum_{i=1}^{m}(\\ln(y_i + 1) - \\ln({\\hat{y}}_i + 1))^2$$\n",
        "\n",
        "Some loss functions for **classification**:\n",
        "\n",
        "Loss function | Equation\n",
        "--- | ---\n",
        "Binary cross-entropy | $$-\\frac{1}{m} \\sum_{i=1}^{m} y_i \\ln(\\hat{y_i}) - (1-y_i)\\ln(1-\\hat{y_i}) $$\n",
        "Hinge loss | $$\\sum_{i=1}^{m} y_i \\max(0,1 - \\hat{y_i}) + (1-y_i)\\max(0, 1+\\hat{y_i}) $$\n",
        "\n",
        "where $y_i$ refers to the true value of the point $i$, $\\hat{y_i}$ is its predicted value and $m$ is the number of training points in our dataset. Of course, the model predictions are function of the parameters, $\\hat{y_i}(\\theta)$. Therefore, our goal is to find $\\theta$ that minimizes our loss function. Cost functions are usually denoted by $J(\\theta)$.\n",
        "\n",
        "Look, for example the next plot for the MSE cost function.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1Umdv8O04EB0_XJZvzZuMf5Xi762_li4B)\n",
        "\n",
        "\n",
        "The main idea of this workshop is to show you some of the most common optimization algorithms used in Machine Learning. In general, all of them work by making a sequence of small steps that reduce the cost function iteratively until a certain tolerance is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDtOKsf_xL_c",
        "colab_type": "text"
      },
      "source": [
        "# **4. First-order methods**\n",
        "\n",
        "The name \"first-order method\" refers to algorithms that use the gradient information of the function, but not the Hessian."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M77fcY-NOP2Z",
        "colab_type": "text"
      },
      "source": [
        "## **4.1 Gradient Descent**\n",
        "\n",
        "Gradient Descent (GD) is an optimization algorithm that moves towards a minimum based on the descent direction using certain step size at each iteration. For this reason, this algorithm (hopefully) converges to a local minimum. \n",
        "\n",
        "In general, for all descent optimization methods, the search process relies mainly on the following two variables:\n",
        "the **direction** in which we need to move and the **step size** that we need to take.\n",
        "\n",
        "### Direction\n",
        "\n",
        "Starting from a random set of parameters $\\theta^{(0)}$, the next step in the GD algorithm is to calculate the gradient of the objective function $J(\\theta)$ at this point,  i.e. $ \\nabla J(\\theta^{(0)})$. Recalling our calculus knowledge, the gradient of a function is related with the slope of the function at the given point. Therefore, since the aim of the algorithm is to minimize, the direction that we need to take has to be the **negative** of the gradient that we have just calculated.\n",
        "\n",
        "Once we know the direction in which we need to move, the next question will be: how far do we need to move, what is the step size that we need to take? For now, let's call this step size $\\alpha$. Therefore, at each iteration, the next position that we need to move to is given by:\n",
        "\n",
        "$\\theta^{(k+1)} = \\theta^{(k)} - \\alpha \\nabla J^{(k)})$\n",
        "\n",
        "### Step size\n",
        "\n",
        "Intuitively, if we choose a step size (a.k.a. **learning rate**) that is too large, we risk to bypass the optimum and never to find it. On the contrary, if we choose a small learning rate, we can remain in the safe zone moving towards the optimum, but this would be extremely inefficient/slow. Then, how do we choose the ideal step size $\\alpha$?\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1zr6eCyJSlAJYD_z9Vx82txNqVj9V_Hss)\n",
        "\n",
        "\n",
        "#### Exact line search\n",
        "\n",
        "Ideally we would need to take the step size that minimizes the funtion at the next point. In other words we would need to solve the following in order to get the optimal step size at each iteration:\n",
        "\n",
        "$\\alpha^* = \\text{argmin}_{\\alpha \\geq 0} ~~~ J \\left( \\theta^{(k)} - \\alpha \\nabla J(\\theta^{(k)}) \\right) $\n",
        "\n",
        "Solving the aboved problem is known as *exact line search*, but obviously this required to solve an adittional optimization problem that can be computationaly expensive in most applications. Therefore, the learning rate $\\alpha$ is most of the times chosen **heuristically** or using other approximations to the exact line search (e.g. backtracking).\n",
        "\n",
        "#### Backtracking line search\n",
        "\n",
        "As mentioned before, backtracking line search is one of the approximation methods of the exact line search, and it relies on two constants $A$ and $B$, such that $0 < A < 0.5$ and $0 < B < 1$. The role of the constant $A$ is to reduce the slope of the line in which the search will be performed (see following plot), and the constant $B$ weights the previous learning rate $\\alpha$.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1K8WDQwVCuQXw7Va2Pev5gVnt6mQTrbVF)\n",
        "\n",
        "At each iteration of the GD algorithm, the backtraking algorithm needs to be run:\n",
        "\n",
        "* Set $\\alpha = 1$\n",
        "* While the following condition holds: $J \\left( \\theta - \\alpha \\nabla J(\\theta) \\right) > J(\\theta) - A \\alpha \\nabla J(\\theta)^T \\nabla J(\\theta)$, reduce $\\alpha$ according to the following: $\\alpha := B \\alpha$. The inequality condition used here is known as the Armijo–Goldstein condition.\n",
        "\n",
        "Typical values for these two constants are $A = [0.01, 0.3]$ and $B = [0.1, 0.8]$.\n",
        "\n",
        "### Stopping criterion\n",
        "\n",
        "The stoping criterion for the GD algorithm is usually specified with a small tolerance $\\epsilon$ for the Euclidean norm of the gradient of the function, i.e. $||\\nabla J(\\theta)||_2 \\leq \\epsilon$.\n",
        "\n",
        "### Algorithm\n",
        "\n",
        "Given the starting point $\\theta^{(0)} \\in \\text{dom} ~ J$; the tolerance $\\epsilon$; the constants $A$ and $B$; and $k=0$\n",
        "\n",
        "1. While $||\\nabla J(\\theta)||_2 > \\epsilon$:\n",
        "2. $~~~~~~$ $\\alpha^{(k)} = 1$ \n",
        "3. $~~~~~~$ While $J \\left( \\theta^{(k)} - \\alpha^{(k)} \\nabla J(\\theta^{(k)}) \\right) > J(\\theta^{(k)}) - A \\alpha^{(k)} \\nabla J(\\theta^{(k)})^T \\nabla J(\\theta^{(k)})$:\n",
        "4. $~~~~~~$ $~~~~~~$ $\\alpha^{(k)} = B \\alpha^{(k)}$\n",
        "5. $~~~~~~$ $\\theta^{(k+1)} = \\theta^{(k)} - \\alpha^{(k)} \\nabla J(\\theta^{(k)})$\n",
        "6. $~~~~~~$ $k = k + 1$\n",
        "\n",
        "##### **Further remarks**\n",
        "\n",
        "In many machine learning applications, we have thousands or millions of samples to train our models. In gradient descent **all** these samples are used to compute the gradient of the cost function (Go back to Section 3 and look at the cost function, if you are not sure why this is true). Therefore, for many interesting applications, computing the gradient $\\nabla_\\theta J(\\theta)$ becomes pretty expensive! For this reason, other variation of gradient descent (a.k.a batch gradient descent) is used, called: \n",
        "\n",
        "* Stochastic Gradient Descent.\n",
        "\n",
        "In this method, instead of using all the training samples to calculate the $\\nabla_\\theta J(\\theta)$, you pass a **single random sample** or a **mini batch of samples** (this is also called: Mini-batch gradient descent). But, the logic behind is identical to what we have review here.\n",
        "\n",
        "##### **Further resources**\n",
        "\n",
        "Chapter 9. *Boyd, S., and Vandenberghe, L., 2004. Convex optimization. Cambridge university press*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iRBg0ivKwJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################\n",
        "# --- Gradient Descent --- #\n",
        "############################\n",
        "\n",
        "def gradient_descent(f, x0, grad_f, lr, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Gradient Descent\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        lr       : Learning rate\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter: \n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#\n",
        "        # Note: you might want to make the above a \"for\" loop with (say) 10k iterations not to run into an infinite loop\n",
        "        grad_i  = 0.       # compute gradient\n",
        "        x       = 0.       # compute step \n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#-#   \n",
        "           \n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using Gradient Descent \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yGtg6oKR1CO",
        "colab_type": "text"
      },
      "source": [
        "Let's use our `gradient_descent` function in a simple function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Fo4x_aSL0M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "9987b237-ae7f-4e5a-a022-1cede1a7a7ef"
      },
      "source": [
        "def Rosenbrock_f(x):\n",
        "    '''\n",
        "    Rosenbrock function\n",
        "    '''\n",
        "    n = np.shape(x)[1]\n",
        "    z = np.sum(100*(x[:,1:] - x[:,:n-1]**2)**2 + (x[:,:n-1] - 1)**2, axis=1)\n",
        "    return z\n",
        "\n",
        "# --- Gradient Descent --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "lr = 0.001\n",
        "xf, x_list, f_list = gradient_descent(Rosenbrock_f, x0, central_finite_diff5, lr, traj=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7d28db487c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mxf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRosenbrock_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentral_finite_diff5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-1138ebf038c7>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(f, x0, grad_f, lr, max_iter, grad_tol, traj)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtraj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mx_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mf_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'flatten'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0sovF3UZ0jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot function\n",
        "x_1 = np.linspace(0,1)\n",
        "x_2 = np.linspace(0,1)\n",
        "X, Y = np.meshgrid(x_1, x_2)\n",
        "Z = Rosenbrock_f(np.append(X.reshape(-1,1), Y.reshape(-1,1), axis=1))\n",
        "Z = Z.reshape(X.shape)\n",
        "\n",
        "x_list    = np.array(x_list).reshape(-1,x0.shape[1])\n",
        "\n",
        "x_summary = []\n",
        "f_summary = []\n",
        "for i in range(x_list.shape[0]):\n",
        "  if i % 100 == 0:\n",
        "    x_summary.append(x_list[i])\n",
        "    f_summary.append(f_list[i])\n",
        "x_summary = np.array(x_summary).reshape(-1,x0.shape[1])\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "display_value = ax.text(0.05, 0.2, '', transform=ax.transAxes)\n",
        "plt.close()\n",
        "\n",
        "def animate(i):\n",
        "    ax.plot(x_summary[:i, 0], x_summary[:i, 1], 'k.', alpha=0.6)    # Animate points\n",
        "    display_value.set_text('Min = ' + str(f_summary[i]))          # Animate display value\n",
        "    ax.set_title('Rosenbrock function, Iteration: ' + str(i*100))  # Animate title\n",
        "    return display_value\n",
        "\n",
        "anim = FuncAnimation(fig, animate, frames=len(f_summary), interval=100, repeat_delay=800)\n",
        "\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-4VzUHKoQHW",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.1 Gradient Descent with line search\n",
        "\n",
        "As you can see, the implementation of GD presented above requires you to give a specific learning rate value. Let's now implement a line search function for determining the learning rate iteratively as part of the training process.\n",
        "\n",
        "<font color='blue'>Code a function that perfomrs **line search**. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDusGePvtfht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################\n",
        "# --- Line search --- #\n",
        "#######################\n",
        "\n",
        "def line_search(grad_i, x, f, A=0.1, B=0.8):\n",
        "    '''\n",
        "    Line search for determining learning rate\n",
        "    INPUTS:\n",
        "        grad_i  : Gradient of function at current position\n",
        "        x       : Current position\n",
        "        f       : Objective function\n",
        "    OUTPUTS:\n",
        "        lr    : Optimal learning rate\n",
        "        iter  : Number of iterations needed in line search\n",
        "    '''\n",
        "    iter = 0\n",
        "    lr   = 1\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#\n",
        "    while 2 > 1: # current point is worst then past point\n",
        "        lr  = 1  # reduce line search\n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "        iter += 1\n",
        "    \n",
        "    return lr, iter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n1w4FkQt8iP",
        "colab_type": "text"
      },
      "source": [
        "Now, let's include our line search function within `gradient_descent` and solve the same optimization problem of the Rosenbrock function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SZ5tAvtuQTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#############################################\n",
        "# --- Gradient Descent with line search --- #\n",
        "#############################################\n",
        "\n",
        "def gradient_descent_ls(f, x0, grad_f, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Gradient Descent with line search\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                  # compute gradient\n",
        "        lr      = line_search(grad_i, x, f)[0] # compute learning rate using line search\n",
        "        x       = x - lr*grad_i                # compute step    \n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using Gradient Descent \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJyt8RM08zMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Gradient Descent with line search --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = gradient_descent_ls(Rosenbrock_f, x0, central_finite_diff5, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTaao-WD-f_5",
        "colab_type": "text"
      },
      "source": [
        "As you can see, when using line search, the number of iterations needed to converge dicreases considerably."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j-3w3QD_S2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with line search')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIKGnby0BiIl",
        "colab_type": "text"
      },
      "source": [
        "You can observe this weird separation between the points. Let's take a look to them closely and you will see the typical **zig-zag** movements of gradient descent. These zig-zag movements cause the algorithm to still last many iterations until convergence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwCUWCYtCN6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig  = plt.figure()\n",
        "plt.plot(x_array[:,0], x_array[:,1], 'g-', lw=0.5)\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with line search')\n",
        "plt.axis([0.19, 0.4, -0.01, 0.2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiNV8TP1DTwL",
        "colab_type": "text"
      },
      "source": [
        "You can imagine this type of movements as going downhill in a valley, similar to the following video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_uGJ-uYFjEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('qPKKtvkVAjY')\n",
        "\n",
        "# Recommendations:\n",
        "# Slow motion to 0.5\n",
        "# Relevant part for us between 0:52 - 1:08"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYXRRfrAHgjG",
        "colab_type": "text"
      },
      "source": [
        "But, actually, in the real life the ball in the video is accumulating **momentum** as it goes down. And, in a way, this momentum is helping the ball to reach the bottom of the hill (the optimum) quicker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-2PhNhFpL8F",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 Gradient Descent with momentum\n",
        "\n",
        "The idea of momentum is to add memory to the optimization algorithm. This memory allows the update of the position, not only based on the current steepest descent direction, but also taking into account the overall descent direction according to the previous steps that the algorithm has taken.\n",
        "\n",
        "Therefore, the position will be updated according to the following:\n",
        "\n",
        "$x^{(k+1)} = x^{(k)} + v^{(k)}$\n",
        "\n",
        "where $v^{(k)}$ is the velocity term defined by:\n",
        "\n",
        "$v^{(k)} = \\beta v^{(k-1)} - \\alpha \\nabla f(x^{(k)}) $\n",
        "\n",
        "where $\\beta \\in [0,1]$ is the momentum hyperparameter communly set to 0.9.\n",
        "##### **Further resources**\n",
        "\n",
        "[MIT open class by Gilbert Strang](https://www.youtube.com/watch?v=wrEcHhoJxjM)\n",
        "\n",
        "\n",
        "<font color='blue'>Code a function for the **momentum** calculation. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PtHtzvDI484",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "# --- Momentum --- #\n",
        "####################\n",
        "\n",
        "def momentum(grad_i, v_prev, lr, beta=0.9):\n",
        "    '''\n",
        "    Momentum function\n",
        "    INPUTS:\n",
        "        grad_i  : Gradient of function at current position\n",
        "        v_prev  : velocity value at the previous position\n",
        "        beta    : Momentum hyperparameter\n",
        "    OUTPUTS:\n",
        "        v       : Velocity term\n",
        "    '''\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#\n",
        "    v = grad_i # implement the weighted sum between the current gradient and the past gradient here\n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "\n",
        "    return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOwQMYLsgEup",
        "colab_type": "text"
      },
      "source": [
        "Now, let's include our momentum function within `gradient_descent_ls` and solve the same optimization problem of the Rosenbrock function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGVVwMIZgAXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################################################\n",
        "# --- Gradient Descent with line search and momentum --- #\n",
        "##########################################################\n",
        "\n",
        "def GD_ls_momentum(f, x0, grad_f, beta=0.3, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Gradient Descent with line search and momentum\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        beta     : Parameter beta for the momentum calculation\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    v_prev = 0      # initialize at zero to get normal GD at first step\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                               # compute gradient\n",
        "        lr      = line_search(grad_i, x, f)[0]              # compute learning rate using line search\n",
        "        v       = momentum(grad_i, v_prev, lr, beta=beta)   # compute momentum\n",
        "        x       = x + v                                     # compute step \n",
        "        v_prev  = v                                         # update previous momentum term   \n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using Gradient Descent with momentum \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYkz4ZUchvB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Gradient Descent with line search and momentum --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = GD_ls_momentum(Rosenbrock_f, x0, central_finite_diff5, beta=0.95, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfUvU9L4iCl_",
        "colab_type": "text"
      },
      "source": [
        "By adding the momentum term we did a huge difference in the number of iterations needed to reach the optimum. Let's look at the steps now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsRWn2gjiOBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with line search and momentum')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOEdaLTzPA_6",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 Nesterov Accelerated Gradient Descent (NAG)\n",
        "\n",
        "The Nesterov Accelerated Gradient Descent (NAG) is a further improvement to the Gradient Descent with momentum algorithm. The step direction in NAG is calculated based on the gradient on an approximated future position instead of the current position, in this way, more gradient information is included into the update step compared to the traditional momentum approach.\n",
        "\n",
        "Therefore, the velocity term in NAG is determined by:\n",
        "\n",
        "$v^{(k)} = \\beta v^{(k-1)} - \\alpha \\nabla f(\\tilde{x}^{(k)})$\n",
        "\n",
        "where $\\tilde{x}^{(k)}$ is the approximated future position that we mentioned before, and is calculated as:\n",
        "\n",
        "$\\tilde{x}^{(k)} = x^{(k)} + \\beta v^{(k-1)}$\n",
        "\n",
        "<font color='blue'>Code a function for the **nesterov** calculation of velocity. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKy5d4Acuz93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "# --- Nesterov --- #\n",
        "####################\n",
        "\n",
        "def nesterov(grad_tilde, v_prev, lr, beta=0.9):\n",
        "    '''\n",
        "    Momentum function\n",
        "    INPUTS:\n",
        "        grad_tilde  : Gradient of function at nesterov modified position\n",
        "        v_prev      : velocity value at the previous position\n",
        "        beta        : Momentum hyperparameter\n",
        "    OUTPUTS:\n",
        "        v           : Velocity term\n",
        "    '''\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-# \n",
        "    v = 0. # Code the direction (equivalent to gradient) term of the Nesterov accelerated gradient here\n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "\n",
        "    return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj5QEyYJ4APq",
        "colab_type": "text"
      },
      "source": [
        "Taking our function `GD_ls_momentum` as a base, <font color='blue'>implement your function **`nesterov` into the `NAG`** function bellow</font> . Then, solve the same optimization problem of the Rosenbrock function using NAG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALcGNjrAvrDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################\n",
        "# --- NAG with line search  --- #\n",
        "#################################\n",
        "\n",
        "def NAG(f, x0, grad_f, beta=0.9, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Nesterov Accelerated Gradient Descent with line search\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        beta     : Parameter beta for the momentum calculation\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    v_prev = 0      # initialize at zero to get normal GD-momentum at first step\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                               # compute gradient at current position\n",
        "        \n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-# \n",
        "        x_tilde = x + v_prev*beta                            # nesterov modified position\n",
        "        g_tilde = grad_f(f,x_tilde)                          # compute gradient of function at x_tilde\n",
        "        lr      = line_search(g_tilde, x, f)[0]              # compute learning rate using line search\n",
        "        v       = nesterov(g_tilde, v_prev, lr, beta=beta)   # compute momentum\n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-# \n",
        "\n",
        "        x       = x + v                                     # compute step \n",
        "        v_prev  = v                                         # update previous momentum term   \n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using NAG \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trwqmVraxOSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- NAG --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = NAG(Rosenbrock_f, x0, central_finite_diff5, beta=0.95, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liriccqy5F5b",
        "colab_type": "text"
      },
      "source": [
        "You can see that NAG improves the convergence further compared to the classical momentum method. Let's look at the trajectory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjlankNU1qpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with NAG')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ROkay11yTTK",
        "colab_type": "text"
      },
      "source": [
        "## **4.4 AdaGrad**\n",
        "\n",
        "We have been using line search as a way to determine a reasonable value for the learning rate. However, in such approach, the learning rate $\\alpha$ is a unique value that is applied in all dimensions of the gradient. Can you see the problem of this? Imagine you have different ranges on each dimension of the gradient, then the learning rate will favour each dimension differently. One straightforward approach would be to have a learning rate for each dimension, i.e. the $\\alpha$ would be a vector instead of a scalar. The problem with this is that in machine learning you can easily end-up with thousand or millions of dimensions (think about a Deep Neural Network). Therefore, one of the first approaches to this problem was AdaGrad, which adaptively scale the learning rate for each dimension. Another advantage is that we do not have to adjust the learning rate manually, as this is adaptively adjusted.\n",
        "\n",
        "The way in which AdaGrad does what we explained above is to update the position according to:\n",
        "\n",
        "$x^{(k)} = x^{(k)} - \\alpha \\frac{\\nabla f(x^{(k)})}{V^{(k)}}$\n",
        "\n",
        "where $V^{(k)}$ is the accumulate historical gradient that is calculated as:\n",
        "\n",
        "$V^{(k)} = \\sqrt{\\sum_{i=1}^{k} \\left(\\nabla f(x^{(k)})\\right)^2 + \\epsilon}$\n",
        "\n",
        "where $\\epsilon$ is just a small number that prevents the division by zero.\n",
        "\n",
        "<font color='blue'>Code a function for the **gradient historical accumulation** term. </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK8LU4WXCWRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################\n",
        "# --- Gradient historical accumulation --- #\n",
        "############################################\n",
        "\n",
        "def grad_accumulation(g_traj, eps=1e-8):\n",
        "    '''\n",
        "    Gradient historical accumulation\n",
        "    Note: A vectorized form might be more efficient, but we will keep it like \n",
        "          this, for educational purposes.\n",
        "    INPUTS:\n",
        "        g_traj  : List of historical gradients\n",
        "        eps     : small number to prevent division by zero\n",
        "    OUTPUTS:\n",
        "        V       : Gradient accumulation\n",
        "    '''\n",
        "    dim    = g_traj[0].shape[1]\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-# \n",
        "    \n",
        "    g_traj = 0. # compile the list of historical gradients here\n",
        "    V      = 0. # compute the gradient accumulation term here\n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "\n",
        "    return V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRVAoko2MMpB",
        "colab_type": "text"
      },
      "source": [
        "Now, let's use your function in AdaGrad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqBh-U1GBr7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "# --- AdaGrad --- #\n",
        "###################\n",
        "\n",
        "def adagrad(f, x0, grad_f, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    AdaGrad optimization algorithm\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    g_traj = []             # To store gradient trajectory\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                  # compute gradient\n",
        "        g_traj.append(grad_i)                  # save gradient\n",
        "        lr      = 0.01                        # learning rate\n",
        "        if iter_i == 0:\n",
        "          x  = x - lr*grad_i                # compute step at first iteration\n",
        "        else:\n",
        "          V  = grad_accumulation(g_traj)\n",
        "          x  = x - lr*grad_i/V              # compute step\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "\n",
        "        # print optimization status\n",
        "        if iter_i % 2000 == 0:\n",
        "          print('Current iteration: ', iter_i)\n",
        "        \n",
        "    print(' Optimization using AdaGrad \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW9tBmxEFq3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- AdaGrad --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = adagrad(Rosenbrock_f, x0, central_finite_diff5, traj=True)#, max_iter=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCQ2dtMNB87Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with AdaGrad')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iv9dIT5M7Km",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the problem becomes evident: **AdaGrad is super slow!**\n",
        "This is because, of course, by reducing the gradient at each iteration the steps that we take are very small. And, since we are accumulating the gradients, we are reducing the gradient by a larger amount at each iteration. Therefore, the nice feature of AdaGrad of taking care of the different ranges in the dimensions of the gradient comes at the expense of very slow convergence.\n",
        "\n",
        "This takes us to an important point in optimization: **Always scale your features!**\n",
        "\n",
        "You can use [standarization or min-max sclaing](https://en.wikipedia.org/wiki/Feature_scaling) for example, but the idea of scaling your features is that you overcome this difficulty of having different ranges in your gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQeN-C1lybf1",
        "colab_type": "text"
      },
      "source": [
        "## **4.3 AdaDelta**\n",
        "\n",
        "AdaDelta is an extension of AdaGrad that tries to overcome the slow convergence by not considering all the accumulated historical gradients, but only the gradients in a certain window over a period. This is accomplished by calculating $V^{(k)}$ as follows:\n",
        "\n",
        "$V^{(k)} = \\sqrt{\\rho V^{(k-1)} + (1-\\rho) \\left(\\nabla f(x^{(k)})\\right)^2 + \\epsilon}$\n",
        "\n",
        "where $\\rho$ is a exponential decay parameter, usually set to 0.95. The reason for this specific form is that, instead of inneficiently store $w$ number of gradients, the same calculation can be obtained as as the exponentially decaying average of the squared gradients that we see in the above expression.\n",
        "\n",
        "In the [original paper](https://arxiv.org/pdf/1212.5701.pdf), the authors noted that the hypothetical units in the update are not consistent. Hence, they defined an accumulation term also for the parameters updates. For this reason, they also definied the accumulation of the parameter updates as follows:\n",
        "\n",
        "$E^{(k)} = \\sqrt{\\rho E^{(k-1)} + (1-\\rho) \\Delta {x^{(k)}}^2 + \\epsilon}$\n",
        "\n",
        "and since  $\\Delta x^{(k)}$ is unknown at the current iteration, it is approximated using the previous position as:\n",
        "\n",
        "$ \\Delta x^{(k)} = - \\frac{E^{(k-1)}}{V^{(k)}} \\nabla f(x^{(k)}) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ1hXHL6ZjEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "# --- AdaDelta --- #\n",
        "####################\n",
        "\n",
        "def adadelta(f, x0, grad_f, rho=0.95, eps=1e-8, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    AdaDelta optimization algorithm\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        rho      : Exponential decay parameter\n",
        "        eps      : Small constant to avoid division over zero\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    E_g = 0; E_x = 0\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                                           # compute gradient\n",
        "        E_g     = rho*E_g + (1-rho)*grad_i*grad_i                       # exponential decay average on gradients\n",
        "        x_delta = - np.sqrt(E_x + eps)*grad_i/np.sqrt(E_g + eps)        # compute x_delta\n",
        "        E_x     = rho*E_x + (1-rho)*x_delta*x_delta                     # exponential decay average on parameters\n",
        "        x       = x + x_delta                                           # compute step\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using AdaDelta \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6toQGLebPsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- AdaDelta --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = adadelta(Rosenbrock_f, x0, central_finite_diff5, rho=0.9, eps=1e-8, max_iter=3000, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqS-70AmI3z6",
        "colab_type": "text"
      },
      "source": [
        "As you can see, for this specific problem, AdaDelta does not do a good job. However, if you set the maximum number of iterations to some number, e.g. 5000, for both AdaGrad and AdaDelta and compare the results, you will see that AdaDelta jumps faster into a zone close to the optimum.\n",
        "\n",
        "The problems on the convergence in AdaDelta has been reported in the literature (e.g. Section 4.4 of this [book](https://books.google.de/books?id=IbnEDwAAQBAJ&pg=PA189&lpg=PA189&dq=adadelta+not+converging&source=bl&ots=f2i8liEovl&sig=ACfU3U2nzVAPCLLtC3Os_cxmHmh7acOBww&hl=en&sa=X&ved=2ahUKEwjl4YLJ7MTqAhWNs4sKHQUfCrQQ6AEwA3oECAgQAQ#v=onepage&q=adadelta%20not%20converging&f=false) and [this paper](https://openreview.net/pdf?id=ryQu7f-RZ)). And even the original paper make the comparison of AdaDelta saying:\n",
        "\n",
        "* *Setting the hyperparameters to $\\epsilon=1^{-6}$ and $\\rho=0.95$ we\n",
        "achieve 2.00% test set error compared to the 2.10% of Schaul\n",
        "et al. While this is nowhere near convergence it gives a sense\n",
        "of how quickly the algorithms can optimize the classification\n",
        "objective.*\n",
        "\n",
        "You might be ask why then AdaDelta is used, even that if it fails to optimize our Rosenbrock function. The answer that comes to my mind is: in many machine learning applications, specially in Deep learning, being close to the optimum is enough for the task we want to accomplish. And if AdaDelta takes as relatively fast to this zone, then maybe is worth considering.\n",
        "\n",
        "Another interesting thing to note here is that even though you do not have a learning rate that has to be tunned, the hyperparameter $\\epsilon$ influences the behaviour of AdaDelta quite significantly for certain problems (e.g. ours here).\n",
        "\n",
        "<font color='blue'>Try different **combinations of the hyperparameters** $\\epsilon$ and $\\rho$ and see their influence in convergence. Also, **compare AdaGrad and AdaDelta** in terms of how far one goes towards the optimum in a fixed number of iterations. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0e-mIURjbM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with AdaDelta')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHOfQge3yfcp",
        "colab_type": "text"
      },
      "source": [
        "## **4.4 RMSProp**\n",
        "\n",
        "RMSProp was proposed by Geoffrey Hinton in his [Coursera lecture](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) and it is pretty much the same as the idea 1 presented at the [AdaDelta paper](https://arxiv.org/pdf/1212.5701.pdf). This idea is: adapt the learning rate using the gradiet information of the previous $w$ steps by computing the average exponential decay. Hence, RMSprop update rule is:\n",
        "\n",
        "$x^{(k)} = x^{(k)} - \\alpha \\frac{\\nabla f(x^{(k)})}{V^{(k)}}$\n",
        "\n",
        "where \n",
        "\n",
        "$V^{(k)} = \\sqrt{\\rho V^{(k-1)} + (1-\\rho) \\left(\\nabla f(x^{(k)})\\right)^2 + \\epsilon}$\n",
        "\n",
        "For the sake of completeness let's also implement it in such form.\n",
        "\n",
        "<font color='blue'>Code the **accumulation of squared gradients** of RMSProp.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKMD9sEzaJGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "# --- RMSProp --- #\n",
        "####################\n",
        "\n",
        "def rmsprop(f, x0, grad_f, rho=0.95, eps=1e-8, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    RMSProp optimization algorithm\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        rho      : Exponential decay parameter\n",
        "        eps      : Small constant to avoid division over zero\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    V = 0\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                                           # compute gradient\n",
        "\n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#\n",
        "        V       = 0. # compute the exponential decay average on gradients\n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#\n",
        "\n",
        "        x       = x - lr*grad_i/V                                       # compute step\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using RMSProp \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p-X2RnGa-AU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- RMSProp --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = rmsprop(Rosenbrock_f, x0, central_finite_diff5, rho=0.9, eps=1e-8, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTU2NeCUbdxB",
        "colab_type": "text"
      },
      "source": [
        "Well, this is a surprise, the algorithm converged to the optimum if we use the RMSProp form! \n",
        "\n",
        "Similar result was encounter in the [book](https://books.google.de/books?id=IbnEDwAAQBAJ&pg=PA189&lpg=PA189&dq=adadelta+not+converging&source=bl&ots=f2i8liEovl&sig=ACfU3U2nzVAPCLLtC3Os_cxmHmh7acOBww&hl=en&sa=X&ved=2ahUKEwjl4YLJ7MTqAhWNs4sKHQUfCrQQ6AEwA3oECAgQAQ#v=onepage&q=rmsprop&f=false) we mentioned aboved. The reason is that the accumulation of the parameters update ($E^{(k-1)}$ in AdaDelta) can act as an accelerator term at the fist iterations. However, when approximating the optimum, this same \"kind of momentum\" prevents the algorithm from convergence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSsT1XGEfbbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with RMSProp')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3PPE0rkylOv",
        "colab_type": "text"
      },
      "source": [
        "## **4.5 Adam**\n",
        "\n",
        "Adam combines the nice key property of the momentum methods with the adaptive learning rate methods. In addition to keep the accumulation term of the squared gradients $V^{(k)}$, Adam also has an accumulation term for past gradients (like the momentum methods).\n",
        "\n",
        "The strategy of Adam is to calculate two moments for the gradients:\n",
        "\n",
        "*   First moment (mean): \n",
        "\n",
        "$~~~~~~~~~~~~~~~~~ m^{(k)}=\\beta_1 m^{(k-1)} + (1-\\beta_1) \\nabla f(x^{(k)})$\n",
        "\n",
        "*   Second moment (uncentered variance):\n",
        "\n",
        "$~~~~~~~~~~~~~~~~~ V^{(k)}= \\beta_2 V^{(k-1)} + (1-\\beta_2) \\left( \\nabla f(x^{(k)}) \\right)^2$\n",
        "\n",
        "where $\\beta_1$ and $\\beta_2$ are exponential decay rates. Recommended values for $\\beta_1$, $\\beta_2$ and $\\epsilon$ are 0.9, 0.999 and $10^{-8}$ respectively.\n",
        "\n",
        "However, the [authors noted](https://arxiv.org/pdf/1412.6980.pdf) that during the first iterations the method is biased towards zero. Terefore, they used bias-corrected moments defined as:\n",
        "\n",
        "*   First bias-corrected moment (mean): \n",
        "\n",
        "$~~~~~~~~~~~~~~~~~ \\hat{m}^{(k)}=\\frac{m^{(k)}}{1- \\beta_1^{k}}$\n",
        "\n",
        "*   Second moment (uncentered variance):\n",
        "\n",
        "$~~~~~~~~~~~~~~~~~ \\hat{V}^{(k)}= \\frac{V^{(k)}}{1- \\beta_2^{k}}$\n",
        "\n",
        "Note the terms $\\beta_1^{k}$ and $\\beta_2^{k}$ are the beta values to the power of the iteration number. Therefore, the update rule for Adam is:\n",
        "\n",
        "$x^{(k+1)} = x^{(k)} - \\alpha \\frac{\\hat{m}^{(k)}}{\\sqrt{\\hat{V}^{(k)}} + \\epsilon}$\n",
        "\n",
        "<font color='blue'>Implement the **moments equations** and the **update rule**.</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCSzAJWNkhqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################\n",
        "# --- Adam --- #\n",
        "################\n",
        "\n",
        "def adam(f, x0, grad_f, lr=0.1, beta_1=0.9, beta_2=0.999, eps=1e-8, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Adam optimization algorithm\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        beta_1   : Exponential decay parameter 1\n",
        "        beta_2   : Exponential decay parameter 2\n",
        "        eps      : Small constant to avoid division over zero\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    m = 0;  V = 0\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                                           # compute gradient\n",
        "\n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#\n",
        "        m       = 0. # compute Moment 1\n",
        "        V       = 0. # compute Moment 2\n",
        "        m_hat   = 0. # compute Biased-corrected moment 1 \n",
        "        V_hat   = 0. # compute Biased-corrected moment 2\n",
        "        x       = x - 0.                   # compute step\n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#\n",
        "\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using Adam \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjxK_C66rv1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Adam --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = adam(Rosenbrock_f, x0, central_finite_diff5, lr=0.05, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8-VxH8svlNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with Adam')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wMJapMKyvET",
        "colab_type": "text"
      },
      "source": [
        "## **4.6 Adamax**\n",
        "\n",
        "Adamax was created by the authors of Adam and is just an Adam version that uses the infinity norm (hence, its name \"max\"), instead of the squared norm. Let's remember the original Adam equation for the second moment:\n",
        "\n",
        "$ V^{(k)}= \\beta_2 V^{(k-1)} + (1-\\beta_2) \\left( \\nabla f(x^{(k)}) \\right)^2$\n",
        "\n",
        "there you can see the squared of the gradient (L-2 norm). Now, Adamax second moment is calculated as:\n",
        "\n",
        "$ V^{(k)}= \\max \\left( \\beta_2 V^{(k-1)}, |\\nabla f(x^{(k)})| \\right)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOaWl136cmm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################\n",
        "# --- Adamax --- #\n",
        "################\n",
        "\n",
        "def adamax(f, x0, grad_f, lr=0.1, beta_1=0.9, beta_2=0.999, eps=1e-8, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Adamax optimization algorithm\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        beta_1   : Exponential decay parameter 1\n",
        "        beta_2   : Exponential decay parameter 2\n",
        "        eps      : Small constant to avoid division over zero\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    m = 0;  V = 0\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                                           # compute gradient\n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#\n",
        "        m       = 0. # compute Moment 1\n",
        "        V       = 0. # compute Moment 2 with L-infinity\n",
        "        m_hat   = 0. # compute Biased-corrected moment 1 \n",
        "        V_hat   = 0. # compute Biased-corrected moment 2\n",
        "        x       = x - 0. # compute step\n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using Adamax \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umr3qowldC1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Adamax --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = adamax(Rosenbrock_f, x0, central_finite_diff5, lr=0.06, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJVhd8MCf-aq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with Adamax')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BuUgUNzgAjT",
        "colab_type": "text"
      },
      "source": [
        "We obtain a very nice performance with this infinity-norm version of Adam: Adamax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8BRiksyypSN",
        "colab_type": "text"
      },
      "source": [
        "## **4.7 Nadam**\n",
        "\n",
        "In a similar way that Adam combines the momentum method with the adaptive learning rate idea, Nadam combines the Adam with NAG. We previously reviewed that NAG is an improved momentum method; therefore, by combining Adam with NAG we expect to have an improved version of Adam. This is done as follows:\n",
        "\n",
        "If we expand the Adam update rule, we have:\n",
        "\n",
        "$x^{(k+1)} = x^{(k)} - \\alpha \\frac{\\frac{\\beta_1 m^{(k-1)} + (1-\\beta_1) \\nabla f(x^{(k)})}{1- \\beta_1^{k}}}{\\sqrt{\\hat{V}^{(k)}} + \\epsilon}$\n",
        "\n",
        "that we can simplify as:\n",
        "\n",
        "$x^{(k+1)} = x^{(k)} - \\frac{\\alpha}{\\sqrt{\\hat{V}^{(k)}} + \\epsilon} \\left(\\frac{\\beta_1 m^{(k-1)}}{1- \\beta_1^{k}} + \\frac{(1-\\beta_1) \\nabla f(x^{(k)})}{1- \\beta_1^{k}} \\right)$\n",
        "\n",
        "Then, similarly to NAG, Nadam uses the current moment $m^{(k)}$ instead of $m^{(k-1)}$, and recalling that $\\hat{m}^{(k)}=\\frac{m^{(k)}}{1- \\beta_1^{k}}$, we obtain the Nadam update rule:\n",
        "\n",
        "$x^{(k+1)} = x^{(k)} - \\frac{\\alpha}{\\sqrt{\\hat{V}^{(k)}} + \\epsilon} \\left(\\beta_1 \\hat{m}^{(k)} + \\frac{(1-\\beta_1) \\nabla f(x^{(k)})}{1- \\beta_1^{k}} \\right)$\n",
        "\n",
        "<font color='blue'>Implement the **Nadam key equations** below.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsyeXxEbT-0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################\n",
        "# --- Nadam --- #\n",
        "#################\n",
        "\n",
        "def nadam(f, x0, grad_f, lr=0.1, beta_1=0.9, beta_2=0.999, eps=1e-8, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Nadam optimization algorithm\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        lr       : Learning rate\n",
        "        beta_1   : Exponential decay parameter 1\n",
        "        beta_2   : Exponential decay parameter 2\n",
        "        eps      : Small constant to avoid division over zero\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        traj     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    m = 0;  V = 0\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                                           # compute gradient\n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#\n",
        "        g_hat   = grad_i/(1-beta_1**(iter_i+1))\n",
        "        m       = 0. # compute Moment 1\n",
        "        m_hat   = 0. # compute Biased-corrected moment 1\n",
        "        V       = 0. # compute Moment 2\n",
        "        V_hat   = 0. # compute Biased-corrected moment 2\n",
        "        m_bar   = 0. # compute compute Nadam m\n",
        "        x       = x - 0. # compute step\n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using Nadam \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjxI_6vrmqpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Nadam --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = nadam(Rosenbrock_f, x0, central_finite_diff5, lr=0.005, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kmzZmoUnZLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with Nadam')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwZUajtrZYgR",
        "colab_type": "text"
      },
      "source": [
        "One take away from the behaviour of Nadam compare to Adam is that there is no such optimizer that works always better for all the problems. In certain problems one optimizer can perform better than the others, whereas the opposite can be true for a different problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETOWsDmkrXm7",
        "colab_type": "text"
      },
      "source": [
        "# **5. Hessian approximation**\n",
        "\n",
        "Similarly as what we did for approximating the gradient using central finite differences, we can approximate the Hessian. For the details and formulas look [here](https://en.wikipedia.org/wiki/Finite_difference).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk3TsAn1lZoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################################################\n",
        "# --- Central second order finite differences --- #\n",
        "###################################################\n",
        "\n",
        "def Second_diff_fxx(f, x):\n",
        "    '''\n",
        "      Central finite differences approximation of Hessian\n",
        "      INPUTS:\n",
        "          f  : Function\n",
        "          x  : Position where to approximate the Hessian\n",
        "      OUTPUTS:\n",
        "          Hxx: Approximation of the Hessian of f at x \n",
        "      '''\n",
        "\n",
        "    dim   = np.shape(x)[1]\n",
        "    # Step-size is taken as the square root of the machine precision\n",
        "    eps  = np.sqrt(np.finfo(float).eps)\n",
        "    Hxx   = np.zeros((dim,dim))\n",
        "    \n",
        "    for j in range(dim):\n",
        "        # compute Fxx (diagonal elements)\n",
        "        x_d_f       = np.copy(x)             # forward step\n",
        "        x_d_b       = np.copy(x)             # backward step\n",
        "        x_d_f[0,j]  = x_d_f[0,j] + eps\n",
        "        x_d_b[0,j]  = x_d_b[0,j] - eps\n",
        "        Hxx[j,j]    = (f(x_d_f) -2*f(x) + f(x_d_b))/eps**2\n",
        "\n",
        "        for i in range(j+1,dim):\n",
        "            # compute Fxy (off-diagonal elements)\n",
        "            # Fxy\n",
        "            x_d_fxfy    = np.copy(x_d_f)\n",
        "            x_d_fxfy[0,i] = x_d_fxfy[0,i] + eps\n",
        "            x_d_fxby    = np.copy(x_d_f)\n",
        "            x_d_fxby[0,i] = x_d_fxby[0,i] - eps\n",
        "            x_d_bxfy    = np.copy(x_d_b)\n",
        "            x_d_bxfy[0,i] = x_d_bxfy[0,i] + eps\n",
        "            x_d_bxby    = np.copy(x_d_b)\n",
        "            x_d_bxby[0,i] = x_d_bxby[0,i] - eps\n",
        "            Hxx[j,i]    = (f(x_d_fxfy) - f(x_d_fxby) - f(x_d_bxfy) + f(x_d_bxby))/(4*eps**2)\n",
        "            Hxx[i,j]    = Hxx[j,i]\n",
        "\n",
        "    return Hxx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ed4DDUYpUV-",
        "colab_type": "text"
      },
      "source": [
        "# **6. Second-order methods**\n",
        "\n",
        "The name \"second-order method\" refers to algorithms that use the Hessian (or an approximation of it) information of the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMMPds2xy8Z_",
        "colab_type": "text"
      },
      "source": [
        "## **6.1 Newton's method**\n",
        "\n",
        "This optimization method calculates the inverse of the Hessian matrix to obtain faster convergence than the first-order gradient descent.\n",
        "\n",
        "The update rule of the Newton's method is:\n",
        "\n",
        "$ x^{(k+1)} = x^{(k)} - \\left(\\nabla^2 f(x^{(k)}) \\right)^{-1}\\nabla f(x^{(k)}) $\n",
        "\n",
        "Similarly as what we did for approximating the gradient using central finite differences, we can approximate the Hessian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLG3Y4gHoT0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x1       = np.array([2.,2.]).reshape(1,-1)\n",
        "Hxx = Second_diff_fxx(Rosenbrock_f, x1)\n",
        "print('Hxx = ',Hxx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX-H60AyhNlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################\n",
        "# --- Newton's Method --- #\n",
        "###########################\n",
        "\n",
        "def newton(f, x0, grad_f, H_f, max_iter=1e3, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Newton's method\n",
        "\n",
        "    Note: the Hessian can become ill-conditioned\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        H_f      : Hessian function\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        traj     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    n      = np.shape(x0)[0]\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # trajectory\n",
        "    if traj == True:\n",
        "            x_list = []\n",
        "            f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:       \n",
        "        grad_i  = grad_f(f,x)                         # compute gradient\n",
        "        Hxx     = Second_diff_fxx(f,x)                # compute Hessian\n",
        "        x       = x - (np.linalg.inv(Hxx)@grad_i.T).T # update              \n",
        "        iter_i += 1\n",
        "        \n",
        "        # trajectory\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "    \n",
        "    print(\" Optimization using Newton's method \\n\")\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "\n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list\n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaN-O4M9lxnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Newton's method --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = newton(Rosenbrock_f, x0, central_finite_diff5, Second_diff_fxx, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPNL9hQQqXm7",
        "colab_type": "text"
      },
      "source": [
        "Of course, using the Hessian information accelerates the optimization significantly! But, this comes at the expense of not only obtaining the Hessian, but also inverting it! This is the reason why second-order methods are not used in practice with large datasets. But, if the dataset is relatively small, go ahead and use second-order methods (whenever you have access to the second derivatives, of course)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAkXjy7Gq8GR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title(\"Rosenbrock with Newton's method\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8sM63RTpjNt",
        "colab_type": "text"
      },
      "source": [
        "## **6.2. BFGS**\n",
        "\n",
        "The Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is a method for optimizing unconstrained problems, such as the one we have been solving here and most of the Machine learning problems. This method belongs to a family of algorithms called \"Quasi-Newton\" methods. The term \"quasi\" appears because instead of needing the exact Hessian, these methods use an approximation of the Hessian or an approximation of its inverse.\n",
        "\n",
        "BFGS uses the following approximation to the inverse of the Hessian:\n",
        "\n",
        "$ {B^{(k+1)}}^{-1} = \\left( I - \\frac{{s^{(k)}}^T y^{(k)}}{\\rho^{(k)}}  \\right) {B^{(k)}}^{-1} \\left( I - \\frac{{y^{(k)}}^T s^{(k)}}{\\rho^{(k)}}  \\right) + \\frac{{s^{(k)}}^T s^{(k)}}{\\rho^{(k)}}$\n",
        "\n",
        "where $B$ represents the Hessian approximation, $I$ is the identity matrix and:\n",
        "\n",
        "$s^{(k)} = x^{(k)} - x^{(k-1)}$\n",
        "\n",
        "$y^{(k)} = \\nabla f(x)^{(k)} - \\nabla f(x)^{(k-1)}$\n",
        "\n",
        "$\\rho^{(k)} = y^{(k)} {s^{(k)}}^T + \\epsilon$\n",
        "\n",
        "<font color='blue'>Implement the **BFGS approximation to the inverse of the Hessian** below.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jj3aLNRCF_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################################################\n",
        "# --- Approximating the inverse of the Hessian --- #\n",
        "####################################################\n",
        "\n",
        "def hessian_bfgs_aprox(x, x_prev, grad_i, grad_i_prev, Hk_prev):\n",
        "    '''\n",
        "    Approximation of the inverse of the Hessian in BFGS\n",
        "\n",
        "    INPUTS:\n",
        "        x           : Current position\n",
        "        x_prev      : Previous position\n",
        "        grad_i      : Current gradient\n",
        "        grad_i_prev : Previous gradient\n",
        "        Hk_prev     : Previous Hessian\n",
        "    OUTPUTS:\n",
        "        Hinv     : Approximation of the inverse of the Hessian\n",
        "    '''\n",
        "    eps  = 1e-8               # Prevent division by zero\n",
        "    dim  = np.shape(x)[1]\n",
        "    I    = np.identity(dim)\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#\n",
        "    sk   = 0. # \n",
        "    yk   = 0. #\n",
        "    rho  = 0. #\n",
        "    Hinv = 0. #\n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#\n",
        "\n",
        "    return Hinv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96_aONhYwRIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################\n",
        "# --- BFGS --- #\n",
        "################\n",
        "\n",
        "def BFGS(f, x0, grad_f, max_iter=1e2, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Optimization algorithm: BFGS\n",
        "    '''\n",
        "    # initialize problem\n",
        "    dim     = np.shape(x0)[1]\n",
        "    x       = np.copy(x0)\n",
        "    iter_i  = 0\n",
        "    eps     = 1e-8\n",
        "    I       = np.identity(dim)\n",
        "    \n",
        "    # trajectory\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # Use first step as gradient descent\n",
        "    grad_i      = grad_f(f,x)\n",
        "    lr          = 0.001\n",
        "    x           = x - lr*grad_i\n",
        "    # -- past values\n",
        "    x_prev      = x0\n",
        "    grad_i_prev = grad_i\n",
        "    # -- new gradient\n",
        "    grad_i      = grad_f(f,x)\n",
        "    # -- past Hessian\n",
        "    sk      = x - x_prev \n",
        "    yk      = grad_i - grad_i_prev\n",
        "    Hk_prev = ((yk@sk.T)/(yk@yk.T))*I\n",
        "\n",
        "    # optimization loop\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                                                    # compute gradient \n",
        "        Hinv    = hessian_bfgs_aprox(x, x_prev, grad_i, grad_i_prev, Hk_prev)    # compute Hessian\n",
        "        x_prev  = x\n",
        "        x       = x - (Hinv@grad_i.T).T\n",
        "\n",
        "        grad_i_prev = grad_i\n",
        "        Hk_prev     = Hinv               \n",
        "        iter_i      += 1 \n",
        "    \n",
        "        # trajectory\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "    \n",
        "    print(\" Optimization using BFGS \\n\")\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i) \n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list\n",
        "           \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ3YLsBX3Nqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- BFGS method --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = BFGS(Rosenbrock_f, x0, central_finite_diff5, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eErq2vHlBNES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title(\"Rosenbrock with BFGS method\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE2eWAP4pk04",
        "colab_type": "text"
      },
      "source": [
        "## **6.3 L-BFGS**\n",
        "\n",
        "The Limited-memory BFGS (L-BFGS) is another \"Quasi-Newton\" method that is very popular in Machine Learning. The special feature about this algorithm is that instead of approximating the complete inverse of the Hessian (with size $n \\times n$, where $n$ is number of dimensions) like BFGS, it approximates a much smaller representation of the approximated Hessian. This key feature make it suitable for problems with many variables. \n",
        "\n",
        "Instead of the approximated inverse Hessian $B^{-1}$, L-BFGS stores a trajectory of the past $m$ updates of the position $x$ and gradient $\\nabla f(x)$. Generally this trajectory storage is limited to less than 10 steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVSOhSNqOjD2",
        "colab_type": "text"
      },
      "source": [
        "# Want to learn more?\n",
        "\n",
        "[Convex Optimization book by Boyd and Vandenberghe](https://web.stanford.edu/~boyd/cvxbook/)\n",
        "\n",
        "[Review of Optimization in Machine Learning](https://arxiv.org/pdf/1906.06821.pdf)\n",
        "\n",
        "[DeepMind-UCL lecture by James Martens](https://www.youtube.com/watch?v=kVU8zTI-Od0)\n",
        "\n",
        "[Summary of optimization methods and Cheat sheet](https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9)\n",
        "\n",
        "**Numerical Optimization** by Jorge Nocedal and Steve Wright\n",
        "\n",
        "[Chapter 8 :Optimization for Training Deep Models](https://www.deeplearningbook.org/contents/optimization.html) of the [Deep Learning book](https://www.deeplearningbook.org/)"
      ]
    }
  ]
}